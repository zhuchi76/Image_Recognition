{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEsmy8fef-QW"
      },
      "source": [
        "## HW3: Decision Tree, AdaBoost and Random Forest\n",
        "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset and test the performance with testing data\n",
        "\n",
        "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling sklearn.tree.DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDRCD35qf-QZ"
      },
      "source": [
        "## Load data\n",
        "The dataset is the Heart Disease Data Set from UCI Machine Learning Repository. It is a binary classifiation dataset, the label is stored in `target` column. **Please note that there exist categorical features which need to be [one-hot encoding](https://www.datacamp.com/community/tutorials/categorical-data) before fit into your model!**\n",
        "See follow links for more information\n",
        "https://archive.ics.uci.edu/ml/datasets/heart+Disease"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnIR1Ar4jNOV",
        "outputId": "2a23a1d9-dbb1-4aae-f374-bcc91a3665da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhxsVzqyf-Qa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
        "df = pd.read_csv(file_url)\n",
        "\n",
        "train_file_past = 'train_idx.npy'\n",
        "#train_file_past = '/content/drive/MyDrive/Image_recognition/HW3/train_idx.npy'\n",
        "train_idx = np.load(train_file_past)\n",
        "\n",
        "test_file_past = 'test_idx.npy'\n",
        "#test_file_past = '/content/drive/MyDrive/Image_recognition/HW3/test_idx.npy'\n",
        "test_idx = np.load(test_file_past)\n",
        "\n",
        "train_df = df.iloc[train_idx]\n",
        "test_df = df.iloc[test_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1wmzLC2f-Qc",
        "outputId": "66e9522d-ea3e-4d4d-fe0e-de52e2596f4f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3ef57e21-5c7b-44e6-afc2-73dd15181a8c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>192</td>\n",
              "      <td>283</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>195</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>reversible</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>170</td>\n",
              "      <td>225</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>146</td>\n",
              "      <td>1</td>\n",
              "      <td>2.8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>fixed</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>221</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>163</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>reversible</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>46</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>120</td>\n",
              "      <td>249</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>144</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>reversible</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>55</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>135</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>161</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ef57e21-5c7b-44e6-afc2-73dd15181a8c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3ef57e21-5c7b-44e6-afc2-73dd15181a8c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3ef57e21-5c7b-44e6-afc2-73dd15181a8c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
              "136   54    1   2       192   283    0        2      195      0      0.0   \n",
              "232   58    0   4       170   225    1        2      146      1      2.8   \n",
              "233   56    1   2       130   221    0        2      163      0      0.0   \n",
              "184   46    1   4       120   249    0        2      144      0      0.8   \n",
              "84    55    0   2       135   250    0        2      161      0      1.4   \n",
              "\n",
              "     slope  ca        thal  target  \n",
              "136      1   1  reversible       0  \n",
              "232      2   2       fixed       1  \n",
              "233      1   0  reversible       0  \n",
              "184      1   0  reversible       0  \n",
              "84       2   0      normal       0  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FngSDoCGDsDk",
        "outputId": "67f7a607-bc07-481e-b0f9-04ab009be14f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'thal': {'fixed': 1, 'normal': 2, 'reversible': 3}}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_single_column(ilocs[0], value, pi)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f55c4d64-247c-4524-aa40-608715a0b556\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>192</td>\n",
              "      <td>283</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>195</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>170</td>\n",
              "      <td>225</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>146</td>\n",
              "      <td>1</td>\n",
              "      <td>2.8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>221</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>163</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>46</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>120</td>\n",
              "      <td>249</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>144</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>55</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>135</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>161</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f55c4d64-247c-4524-aa40-608715a0b556')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f55c4d64-247c-4524-aa40-608715a0b556 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f55c4d64-247c-4524-aa40-608715a0b556');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
              "136   54    1   2       192   283    0        2      195      0      0.0   \n",
              "232   58    0   4       170   225    1        2      146      1      2.8   \n",
              "233   56    1   2       130   221    0        2      163      0      0.0   \n",
              "184   46    1   4       120   249    0        2      144      0      0.8   \n",
              "84    55    0   2       135   250    0        2      161      0      1.4   \n",
              "\n",
              "     slope  ca  thal  target  \n",
              "136      1   1     3       0  \n",
              "232      2   2     1       1  \n",
              "233      1   0     3       0  \n",
              "184      1   0     3       0  \n",
              "84       2   0     2       0  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = train_df['thal'].astype('category').cat.categories.tolist()\n",
        "replace_map_comp = {'thal' : {k: v for k,v in zip(labels,list(range(1,len(labels)+1)))}}\n",
        "print(replace_map_comp)\n",
        "train_df.replace(replace_map_comp, inplace=True)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm8LlxwanjRR",
        "outputId": "5faac207-f326-438a-b964-8b43d6eba7e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[54.  1.  2. ...  1.  1.  3.]\n",
            " [58.  0.  4. ...  2.  2.  1.]\n",
            " [56.  1.  2. ...  1.  0.  3.]\n",
            " ...\n",
            " [64.  1.  1. ...  2.  0.  3.]\n",
            " [44.  1.  3. ...  1.  0.  2.]\n",
            " [57.  1.  3. ...  1.  0.  2.]]\n",
            "[0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1\n",
            " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1\n",
            " 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0\n",
            " 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0\n",
            " 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1\n",
            " 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0]\n",
            "[138  63]\n"
          ]
        }
      ],
      "source": [
        "train_df_data = train_df.values[:,0:-1]\n",
        "print(train_df_data)\n",
        "train_df_target = train_df['target'].astype(int).values[:]\n",
        "print(train_df_target)\n",
        "print(np.bincount(train_df_target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1JBRErDoT-b",
        "outputId": "58a81d41-4900-480d-c33b-c81c26ff3589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[58.  0.  1. ...  1.  0.  2.]\n",
            " [50.  0.  3. ...  2.  0.  2.]\n",
            " [57.  0.  4. ...  1.  0.  2.]\n",
            " ...\n",
            " [57.  1.  4. ...  1.  0.  3.]\n",
            " [39.  1.  3. ...  1.  0.  2.]\n",
            " [70.  1.  4. ...  2.  3.  2.]]\n",
            "[0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1\n",
            " 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_single_column(ilocs[0], value, pi)\n"
          ]
        }
      ],
      "source": [
        "labels = test_df['thal'].astype('category').cat.categories.tolist()\n",
        "replace_map_comp = {'thal' : {k: v for k,v in zip(labels,list(range(1,len(labels)+1)))}}\n",
        "test_df.replace(replace_map_comp, inplace=True)\n",
        "\n",
        "test_df_data = test_df.values[:,0:-1]\n",
        "print(test_df_data)\n",
        "test_df_target = test_df['target'].astype(int).values[:]\n",
        "print(test_df_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvFqH59Zf-Qd"
      },
      "source": [
        "## Question 1\n",
        "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEJGHMNXf-Qd"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def class_counts(rows):\n",
        "    \"\"\"Counts the number of each type of example in a dataset.\"\"\"\n",
        "    counts = {}  # a dictionary of label -> count.\n",
        "    for row in rows:\n",
        "        # in our dataset format, the label is always the last column\n",
        "        label = row # label = row[-1]\n",
        "        if label not in counts:\n",
        "            counts[label] = 0\n",
        "        counts[label] += 1\n",
        "    return counts\n",
        "def gini(sequence):\n",
        "    # Counts the number of each type of example in a dataset.\n",
        "    counts = class_counts(sequence)\n",
        "    # Calculate impurity\n",
        "    impurity = 1\n",
        "    for lbl in counts:\n",
        "        prob_of_lbl = counts[lbl] / float(len(sequence))\n",
        "        impurity -= prob_of_lbl**2\n",
        "    return impurity\n",
        "\n",
        "def entropy(sequence):\n",
        "    # Counts the number of each type of example in a dataset.\n",
        "    counts = class_counts(sequence)\n",
        "    # Calculate entropy\n",
        "    entropy = 0\n",
        "    for lbl in counts:\n",
        "        prob_of_lbl = counts[lbl] / float(len(sequence))\n",
        "        entropy -= prob_of_lbl * math.log(prob_of_lbl, 2)\n",
        "    return entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbihxeYif-Qe"
      },
      "outputs": [],
      "source": [
        "# 1 = class 1,\n",
        "# 2 = class 2\n",
        "data = np.array([1,2,1,1,1,1,2,2,1,1,2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vydeNDpf-Qf",
        "outputId": "89c97af4-1844-483a-cd16-7737f9a74970"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gini of data is  0.4628099173553719\n"
          ]
        }
      ],
      "source": [
        "print(\"Gini of data is \", gini(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzqWDaalf-Qg",
        "outputId": "b6764d2d-e72d-4797-946b-f2f6e43bc8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entropy of data is  0.9456603046006402\n"
          ]
        }
      ],
      "source": [
        "print(\"Entropy of data is \", entropy(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkcuyi1Jf-Qh"
      },
      "source": [
        "## Question 2\n",
        "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the test data. You should implement two arguments for the Decision Tree algorithm\n",
        "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
        "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oDNK3MYBaVr"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "    \n",
        "    def is_leaf(self):\n",
        "        return self.value is not None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEdzyhYcf-Qh"
      },
      "outputs": [],
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, criterion='gini', max_depth=None, min_samples_split=2, max_features=None):\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_features = max_features\n",
        "        self.root = None\n",
        "        return None\n",
        "    \n",
        "    def _is_finished(self, depth):\n",
        "        if (depth >= self.max_depth\n",
        "            or self.n_class_labels == 1\n",
        "            or self.n_samples < self.min_samples_split):\n",
        "            return True\n",
        "        return False\n",
        "    \n",
        "    def _entropy_or_gini(self, y):\n",
        "        if(self.criterion == 'gini'):\n",
        "            return gini(y)\n",
        "        else: return entropy(y)\n",
        "\n",
        "\n",
        "    def _create_split(self, X, thresh):\n",
        "        left_idx = np.argwhere(X <= thresh).flatten()\n",
        "        right_idx = np.argwhere(X > thresh).flatten()\n",
        "        return left_idx, right_idx\n",
        "\n",
        "    def _information_gain(self, X, y, thresh):\n",
        "        parent_loss = self._entropy_or_gini(y)\n",
        "        left_idx, right_idx = self._create_split(X, thresh)\n",
        "        n, n_left, n_right = len(y), len(left_idx), len(right_idx)\n",
        "\n",
        "        if n_left == 0 or n_right == 0: \n",
        "            return 0\n",
        "        \n",
        "        child_loss = (n_left / n) * self._entropy_or_gini(y[left_idx]) + (n_right / n) * self._entropy_or_gini(y[right_idx])\n",
        "        return parent_loss - child_loss\n",
        "\n",
        "    def _best_split(self, X, y, features):\n",
        "        split = {'score':- 1, 'feat': None, 'thresh': None}\n",
        "\n",
        "        for feat in features:\n",
        "            X_feat = X[:, feat]\n",
        "            thresholds = np.unique(X_feat)\n",
        "            for thresh in thresholds:\n",
        "                score = self._information_gain(X_feat, y, thresh)\n",
        "\n",
        "                if score > split['score']:\n",
        "                    split['score'] = score\n",
        "                    split['feat'] = feat\n",
        "                    split['thresh'] = thresh\n",
        "\n",
        "        return split['feat'], split['thresh']\n",
        "    \n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        self.n_samples, self.n_features = X.shape\n",
        "        self.n_class_labels = len(np.unique(y))\n",
        "\n",
        "        # stopping criteria\n",
        "        if self._is_finished(depth):\n",
        "            most_common_Label = np.argmax(np.bincount(y))\n",
        "            return Node(value=most_common_Label)\n",
        "\n",
        "        # get best split\n",
        "        if self.max_features == None:\n",
        "            self.max_features = self.n_features\n",
        "        rnd_feats = np.random.choice(self.n_features, int(self.max_features), replace=False)\n",
        "        best_feat, best_thresh = self._best_split(X, y, rnd_feats)\n",
        "        # grow children recursively\n",
        "        left_idx, right_idx = self._create_split(X[:, best_feat], best_thresh)\n",
        "        left_child = self._build_tree(X[left_idx, :], y[left_idx], depth + 1)\n",
        "        right_child = self._build_tree(X[right_idx, :], y[right_idx], depth + 1)\n",
        "        # print('best feature = {}, best_threshold = {}'.format(best_feat, best_thresh))\n",
        "        \n",
        "        return Node(best_feat, best_thresh, left_child, right_child)\n",
        "    \n",
        "    def _traverse_tree(self, x, node):\n",
        "        if node.is_leaf():\n",
        "            return node.value\n",
        "        \n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)\n",
        "     \n",
        "    def fit(self, X, y):\n",
        "        self.root = self._build_tree(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = [self._traverse_tree(x, self.root) for x in X]\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def _collect_feature(self, node):\n",
        "        if node.is_leaf():\n",
        "            # print(node.feature)\n",
        "            return { node.feature : 1 }\n",
        "\n",
        "        lf = self._collect_feature(node.left)\n",
        "        rt = self._collect_feature(node.right)\n",
        "\n",
        "        all_feature = { node.feature : 1 }\n",
        "        for lf_feature in lf:\n",
        "            if not lf_feature in all_feature:\n",
        "                all_feature[lf_feature] = lf[lf_feature]\n",
        "            else: all_feature[lf_feature] += lf[lf_feature]\n",
        "        for rt_feature in rt:\n",
        "            if not rt_feature in all_feature:\n",
        "                all_feature[rt_feature] = rt[rt_feature]\n",
        "            else: all_feature[rt_feature] += rt[rt_feature]\n",
        "        return all_feature\n",
        "\n",
        "    def _count_feature(self):\n",
        "        return self._collect_feature(self.root)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdASmDDRf-Qi"
      },
      "source": [
        "### Question 2.1\n",
        "Using `criterion=gini`, showing the accuracy score of test data by `max_depth=3` and `max_depth=10`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBjdzIG_WN3x"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egNqWZygoplO",
        "outputId": "d899b624-5d28-4467-fe41-72c2ee5f4c19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.79\n"
          ]
        }
      ],
      "source": [
        "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
        "clf_depth3.fit(train_df_data, train_df_target)\n",
        "\n",
        "pred_target = clf_depth3.predict(test_df_data)\n",
        "acc = accuracy_score(test_df_target, pred_target)\n",
        "\n",
        "print(\"Accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EE_hyzHf-Qi",
        "outputId": "9b7f72c8-b28c-4b8d-c7f5-55dfde451531"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.73\n"
          ]
        }
      ],
      "source": [
        "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)\n",
        "clf_depth10.fit(train_df_data, train_df_target)\n",
        "\n",
        "pred_target = clf_depth10.predict(test_df_data)\n",
        "acc = accuracy_score(test_df_target, pred_target)\n",
        "\n",
        "print(\"Accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVlghU2Wf-Qi"
      },
      "source": [
        "### Question 2.2\n",
        "Using `max_depth=3`, showing the accuracy score of test data by `criterion=gini` and `criterion=entropy`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snYsGSOjWhcg",
        "outputId": "0eb762c6-8c80-4239-a756-7f64849a7db9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.78\n"
          ]
        }
      ],
      "source": [
        "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
        "clf_gini.fit(train_df_data, train_df_target)\n",
        "\n",
        "pred_target = clf_gini.predict(test_df_data)\n",
        "acc = accuracy_score(test_df_target, pred_target)\n",
        "\n",
        "print(\"Accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLNy8wxmWqFn",
        "outputId": "8d9bbcce-49c9-4ee6-9b2d-9385d2fa756b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.76\n"
          ]
        }
      ],
      "source": [
        "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)\n",
        "clf_entropy.fit(train_df_data, train_df_target)\n",
        "\n",
        "y_pred = clf_entropy.predict(test_df_data)\n",
        "acc = accuracy_score(test_df_target, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe4pfNcAf-Qj"
      },
      "source": [
        "- Note: Your decisition tree scores should over **0.7**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
        "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
        "- Hint: You can use the recursive method to build the nodes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsR44nLef-Qj"
      },
      "source": [
        "## Question 3\n",
        "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
        "\n",
        "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
        "\n",
        "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZccVdQclJYE",
        "outputId": "5a044deb-684f-4240-af13-d78ec552d597"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 2, 10: 2, 11: 1, 12: 3, 2: 1, 3: 2, 4: 5, 6: 1, 7: 5, 8: 1, 9: 3, None: 27}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf_gini._count_feature()\n",
        "clf_depth10._count_feature()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO3SM-K3pgpZ",
        "outputId": "c0cb5250-c964-4f17-9103-14f3e3951146"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n"
          ]
        }
      ],
      "source": [
        "ft = train_df.columns.tolist()\n",
        "ft.remove('target')\n",
        "print(ft)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3stJvbzoJhE",
        "outputId": "f28a5f25-d191-4db7-aa86-7a919f6dd003"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 0, 1, 2, 5, 0, 1, 5, 1, 3, 2, 1, 3]\n"
          ]
        }
      ],
      "source": [
        "ft_val = []\n",
        "for i in range(13):\n",
        "    ft_val.append(0)\n",
        "# count_feature = clf_gini._count_feature()\n",
        "count_feature = clf_depth10._count_feature()\n",
        "del count_feature[None]\n",
        "for feature_idx in count_feature:\n",
        "    if feature_idx == None:\n",
        "        ft_val[-1] = count_feature[feature_idx]\n",
        "    else:   ft_val[feature_idx] = count_feature[feature_idx]\n",
        "print(ft_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVJGN_VGnIwA",
        "outputId": "6b5ba67e-fb2e-4faa-edb3-eb6608c871f9"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "horiz = np.array(ft)\n",
        "height = np.array(ft_val)\n",
        "\n",
        "plt.barh(horiz, height)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1QA8QEnf-Qk"
      },
      "source": [
        "## Question 4\n",
        "implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n",
        "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cKLYUB6f-Ql"
      },
      "source": [
        "### Question 4.1\n",
        "Show the accuracy score of test data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPLBqpvHnTtr"
      },
      "outputs": [],
      "source": [
        "# change target value 0 to -1\n",
        "train_df_target = np.where(train_df_target == 0, -1, train_df_target)\n",
        "test_df_target = np.where(test_df_target == 0, -1, test_df_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-Ad1JzLspBZ"
      },
      "outputs": [],
      "source": [
        "# Decision stump used as weak classifier\n",
        "class DecisionStump:\n",
        "    def __init__(self):\n",
        "        self.polarity = 1\n",
        "        self.feature_idx = None\n",
        "        self.threshold = None\n",
        "        self.alpha = None\n",
        "\n",
        "    def predict(self, X):\n",
        "        n_samples = X.shape[0]\n",
        "        X_column = X[:, self.feature_idx]\n",
        "        predictions = np.ones(n_samples)\n",
        "        if self.polarity == 1:\n",
        "            predictions[X_column < self.threshold] = -1\n",
        "        else:\n",
        "            predictions[X_column > self.threshold] = -1\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "class Adaboost:\n",
        "    def __init__(self, n_estimators=5):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.clfs = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize weights to 1/N\n",
        "        w = np.full(n_samples, (1 / n_samples))\n",
        "\n",
        "        self.clfs = []\n",
        "\n",
        "        # Iterate through classifiers\n",
        "        for _ in range(self.n_estimators):\n",
        "            clf = DecisionStump()\n",
        "            min_error = float(\"inf\")\n",
        "\n",
        "            # greedy search to find best threshold and feature\n",
        "            for feature_i in range(n_features):\n",
        "                X_column = X[:, feature_i]\n",
        "                thresholds = np.unique(X_column)\n",
        "\n",
        "                for threshold in thresholds:\n",
        "                    # predict with polarity 1\n",
        "                    p = 1\n",
        "                    predictions = np.ones(n_samples)\n",
        "                    predictions[X_column < threshold] = -1\n",
        "\n",
        "                    # Error = sum of weights of misclassified samples\n",
        "                    misclassified = w[y != predictions]\n",
        "                    error = sum(misclassified)\n",
        "\n",
        "                    if error > 0.5:\n",
        "                        error = 1 - error\n",
        "                        p = -1\n",
        "\n",
        "                    # store the best configuration\n",
        "                    if error < min_error:\n",
        "                        clf.polarity = p\n",
        "                        clf.threshold = threshold\n",
        "                        clf.feature_idx = feature_i\n",
        "                        min_error = error\n",
        "\n",
        "            # calculate alpha\n",
        "            EPS = 1e-10\n",
        "            clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\n",
        "\n",
        "            # calculate predictions and update weights\n",
        "            predictions = clf.predict(X)\n",
        "\n",
        "            w *= np.exp(-clf.alpha * y * predictions)\n",
        "            # Normalize to one\n",
        "            w /= np.sum(w)\n",
        "\n",
        "            # Save classifier\n",
        "            self.clfs.append(clf)\n",
        "\n",
        "    def predict(self, X):\n",
        "        clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]\n",
        "        y_pred = np.sum(clf_preds, axis=0)\n",
        "        y_pred = np.sign(y_pred)\n",
        "\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4v24JSElTW-",
        "outputId": "defb2633-d027-4510-9e65-d0940adfbc62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AdaBoost Accuracy: 0.83\n"
          ]
        }
      ],
      "source": [
        "# Fit model\n",
        "clf_10ab = Adaboost( n_estimators = 10 )\n",
        "clf_10ab.fit(train_df_data, train_df_target)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "# Predict on test set\n",
        "pred_target = clf_10ab.predict(test_df_data)\n",
        "acc = accuracy_score(test_df_target, pred_target)\n",
        "\n",
        "print(\"AdaBoost Accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ7LZWDjnQ2z",
        "outputId": "1f39b6b3-3bb9-4572-8365-5758bff48c7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AdaBoost Accuracy: 0.81\n"
          ]
        }
      ],
      "source": [
        "# Fit model\n",
        "clf_100ab = Adaboost( n_estimators = 100 )\n",
        "clf_100ab.fit(train_df_data, train_df_target)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "# Predict on test set\n",
        "pred_target = clf_100ab.predict(test_df_data)\n",
        "acc = accuracy_score(test_df_target, pred_target)\n",
        "\n",
        "print(\"AdaBoost Accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIksRG8qf-Ql"
      },
      "source": [
        "## Question 5\n",
        "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
        "\n",
        "1. **n_estimators**: The number of trees in the forest. \n",
        "2. **max_features**: The number of random select features to consider when looking for the best split\n",
        "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM0Tgm9z5Xcm"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "class RandomForest:\n",
        "    '''\n",
        "    A class that implements Random Forest algorithm from scratch.\n",
        "    '''\n",
        "    def __init__(self, n_estimators, max_features, boostrap=True, criterion='gini', min_samples_split=2, max_depth=5):\n",
        "        self.num_trees = n_estimators\n",
        "        self.max_features = max_features\n",
        "        self.boostrap = boostrap\n",
        "        self.critersion = criterion\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        # Will store individually trained decision trees\n",
        "        self.decision_trees = []\n",
        "        \n",
        "    @staticmethod\n",
        "    def _sample(X, y):\n",
        "        '''\n",
        "        Helper function used for boostrap sampling.\n",
        "        \n",
        "        :param X: np.array, features\n",
        "        :param y: np.array, target\n",
        "        :return: tuple (sample of features, sample of target)\n",
        "        '''\n",
        "        n_rows, n_cols = X.shape\n",
        "        # Sample with replacement\n",
        "        samples = np.random.choice(a=n_rows, size=n_rows, replace=True)\n",
        "        return X[samples], y[samples]\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        '''\n",
        "        Trains a Random Forest classifier.\n",
        "        \n",
        "        :param X: np.array, features\n",
        "        :param y: np.array, target\n",
        "        :return: None\n",
        "        '''\n",
        "        # Reset\n",
        "        if len(self.decision_trees) > 0:\n",
        "            self.decision_trees = []\n",
        "            \n",
        "        # Build each tree of the forest\n",
        "        num_built = 0\n",
        "        while num_built < self.num_trees:\n",
        "            try:\n",
        "                #print(num_built)\n",
        "                clf = DecisionTree( criterion=self.critersion, \n",
        "                    max_depth=self.max_depth, \n",
        "                    min_samples_split=self.min_samples_split\n",
        "                )\n",
        "                \n",
        "                # Obtain data sample\n",
        "                if self.boostrap:\n",
        "                    _X, _y = self._sample(X, y)\n",
        "                # Train\n",
        "                clf.fit(_X, _y)\n",
        "                # Save the classifier\n",
        "                self.decision_trees.append(clf)\n",
        "                num_built += 1\n",
        "            except Exception as e:\n",
        "                continue\n",
        "    \n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Predicts class labels for new data instances.\n",
        "        \n",
        "        :param X: np.array, new instances to predict\n",
        "        :return: \n",
        "        '''\n",
        "        # Make predictions with every tree in the forest\n",
        "        y = []\n",
        "        for tree in self.decision_trees:\n",
        "            y.append(tree.predict(X))\n",
        "        \n",
        "        # Reshape so we can find the most common value\n",
        "        y = np.swapaxes(a=y, axis1=0, axis2=1)\n",
        "        \n",
        "        # Use majority voting for the final prediction\n",
        "        predictions = []\n",
        "        for preds in y:\n",
        "            counter = Counter(preds)\n",
        "            predictions.append(counter.most_common(1)[0][0])\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8_TjqUff-Qm"
      },
      "source": [
        "### Question 5.1\n",
        "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of test data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTZwwHLRUclA"
      },
      "outputs": [],
      "source": [
        "# change target value back from -1 to 0\n",
        "train_df_target = np.where(train_df_target == -1, 0, train_df_target)\n",
        "test_df_target = np.where(test_df_target == -1, 0, test_df_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoyZ4vpGf-Qm",
        "outputId": "53eed5f5-31f8-4df6-ab07-284435b17c4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForest accuracy:  0.7\n"
          ]
        }
      ],
      "source": [
        "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(train_df_data.shape[1]))\n",
        "clf_10tree.fit(train_df_data, train_df_target)\n",
        "pred_target = clf_10tree.predict(test_df_data)\n",
        "acc = accuracy_score(test_df_target, pred_target)\n",
        "print('RandomForest accuracy: ',acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0IuabolTHAB",
        "outputId": "b6b50387-5185-4d04-a10c-753dd4c49316"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForest accuracy:  0.8\n"
          ]
        }
      ],
      "source": [
        "clf_100tree = RandomForest(n_estimators=100, max_features=np.sqrt(train_df_data.shape[1]))\n",
        "clf_100tree.fit(train_df_data, train_df_target)\n",
        "pred_target = clf_100tree.predict(test_df_data)\n",
        "acc = accuracy_score(test_df_target, pred_target)\n",
        "print('RandomForest accuracy: ',acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6mdWSFzf-Qm"
      },
      "source": [
        "### Question 5.2\n",
        "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of test data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbX5HlwXf-Qm",
        "outputId": "1554df86-2407-4fb7-db3e-f56234bb0376"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForest accuracy:  0.8\n"
          ]
        }
      ],
      "source": [
        "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(train_df_data.shape[1]))\n",
        "clf_random_features.fit(train_df_data, train_df_target)\n",
        "pred_target = clf_random_features.predict(test_df_data)\n",
        "acc = accuracy_score(test_df_target, pred_target)\n",
        "print('RandomForest accuracy: ',acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zWrL-SCsr0a",
        "outputId": "f3295216-59b6-4a10-af9a-cd44ceccd9df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForest accuracy:  0.77\n"
          ]
        }
      ],
      "source": [
        "clf_all_features = RandomForest(n_estimators=10, max_features=train_df_data.shape[1])\n",
        "clf_all_features.fit(train_df_data, train_df_target)\n",
        "pred_target = clf_all_features.predict(test_df_data)\n",
        "acc = accuracy_score(test_df_target, pred_target)\n",
        "print('RandomForest accuracy: ',acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjESv32tf-Qn"
      },
      "source": [
        "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jhz1GJULf-Qn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq19MgK1f-Qn"
      },
      "source": [
        "### Question 6.\n",
        "Try you best to get highest test accuracy score by \n",
        "- Feature engineering\n",
        "- Hyperparameter tuning\n",
        "- Implement any other ensemble methods, such as gradient boosting. Please note that you cannot call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHuc6GbmI0-b"
      },
      "source": [
        "## Helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTe6NsZ0EZ2C"
      },
      "outputs": [],
      "source": [
        "class Sigmoid():\n",
        "    def __call__(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def gradient(self, x):\n",
        "        return self.__call__(x) * (1 - self.__call__(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAlaDAqWFx3u"
      },
      "outputs": [],
      "source": [
        "class Loss(object):\n",
        "    def loss(self, y_true, y_pred):\n",
        "        return NotImplementedError()\n",
        "\n",
        "    def gradient(self, y, y_pred):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def acc(self, y, y_pred):\n",
        "        return 0\n",
        "\n",
        "class SquareLoss(Loss):\n",
        "    def __init__(self): pass\n",
        "\n",
        "    def loss(self, y, y_pred):\n",
        "        return 0.5 * np.power((y - y_pred), 2)\n",
        "\n",
        "    def gradient(self, y, y_pred):\n",
        "        return -(y - y_pred)\n",
        "\n",
        "class CrossEntropy(Loss):\n",
        "    def __init__(self): pass\n",
        "\n",
        "    def loss(self, y, p):\n",
        "        # Avoid division by zero\n",
        "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "        return - y * np.log(p) - (1 - y) * np.log(1 - p)\n",
        "\n",
        "    def acc(self, y, p):\n",
        "        return accuracy_score(np.argmax(y, axis=1), np.argmax(p, axis=1))\n",
        "\n",
        "    def gradient(self, y, p):\n",
        "        # Avoid division by zero\n",
        "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "        return - (y / p) + (1 - y) / (1 - p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bpqv9OdrHgnc"
      },
      "outputs": [],
      "source": [
        "def calculate_variance(X):\n",
        "    \"\"\" Return the variance of the features in dataset X \"\"\"\n",
        "    mean = np.ones(np.shape(X)) * X.mean(0)\n",
        "    n_samples = np.shape(X)[0]\n",
        "    variance = (1 / n_samples) * np.diag((X - mean).T.dot(X - mean))\n",
        "    \n",
        "    return variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6qgpOfWIf5v"
      },
      "outputs": [],
      "source": [
        "def to_categorical(x, n_col=None):\n",
        "    \"\"\" One-hot encoding of nominal values \"\"\"\n",
        "    if not n_col:\n",
        "        n_col = np.amax(x) + 1\n",
        "    one_hot = np.zeros((x.shape[0], n_col))\n",
        "    one_hot[np.arange(x.shape[0]), x] = 1\n",
        "    return one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST8sDCq2SxHx"
      },
      "outputs": [],
      "source": [
        "def divide_on_feature(X, feature_i, threshold):\n",
        "    \"\"\" Divide dataset based on if sample value on feature index is larger than\n",
        "        the given threshold \"\"\"\n",
        "    split_func = None\n",
        "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
        "        split_func = lambda sample: sample[feature_i] >= threshold\n",
        "    else:\n",
        "        split_func = lambda sample: sample[feature_i] == threshold\n",
        "\n",
        "    X_1 = np.array([sample for sample in X if split_func(sample)])\n",
        "    X_2 = np.array([sample for sample in X if not split_func(sample)])\n",
        "\n",
        "    return np.array([X_1, X_2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUk4m3RTI8so"
      },
      "source": [
        "## New Decision tree model for Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI8iqis0Rwgh"
      },
      "outputs": [],
      "source": [
        "class DecisionNode():\n",
        "    \"\"\"Class that represents a decision node or leaf in the decision tree\n",
        "    Parameters:\n",
        "    -----------\n",
        "    feature_i: int\n",
        "        Feature index which we want to use as the threshold measure.\n",
        "    threshold: float\n",
        "        The value that we will compare feature values at feature_i against to\n",
        "        determine the prediction.\n",
        "    value: float\n",
        "        The class prediction if classification tree, or float value if regression tree.\n",
        "    true_branch: DecisionNode\n",
        "        Next decision node for samples where features value met the threshold.\n",
        "    false_branch: DecisionNode\n",
        "        Next decision node for samples where features value did not meet the threshold.\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_i=None, threshold=None,\n",
        "                 value=None, true_branch=None, false_branch=None):\n",
        "        self.feature_i = feature_i          # Index for the feature that is tested\n",
        "        self.threshold = threshold          # Threshold value for feature\n",
        "        self.value = value                  # Value if the node is a leaf in the tree\n",
        "        self.true_branch = true_branch      # 'Left' subtree\n",
        "        self.false_branch = false_branch    # 'Right' subtree\n",
        "\n",
        "\n",
        "# Super class of RegressionTree and ClassificationTree\n",
        "class DecisionTree(object):\n",
        "    \"\"\"Super class of RegressionTree and ClassificationTree.\n",
        "    Parameters:\n",
        "    -----------\n",
        "    min_samples_split: int\n",
        "        The minimum number of samples needed to make a split when building a tree.\n",
        "    min_impurity: float\n",
        "        The minimum impurity required to split the tree further.\n",
        "    max_depth: int\n",
        "        The maximum depth of a tree.\n",
        "    loss: function\n",
        "        Loss function that is used for Gradient Boosting models to calculate impurity.\n",
        "    \"\"\"\n",
        "    def __init__(self, min_samples_split=2, min_impurity=1e-7,\n",
        "                 max_depth=float(\"inf\"), loss=None):\n",
        "        self.root = None  # Root node in dec. tree\n",
        "        # Minimum n of samples to justify split\n",
        "        self.min_samples_split = min_samples_split\n",
        "        # The minimum impurity to justify split\n",
        "        self.min_impurity = min_impurity\n",
        "        # The maximum depth to grow the tree to\n",
        "        self.max_depth = max_depth\n",
        "        # Function to calculate impurity (classif.=>info gain, regr=>variance reduct.)\n",
        "        self._impurity_calculation = None\n",
        "        # Function to determine prediction of y at leaf\n",
        "        self._leaf_value_calculation = None\n",
        "        # If y is one-hot encoded (multi-dim) or not (one-dim)\n",
        "        self.one_dim = None\n",
        "        # If Gradient Boost\n",
        "        self.loss = loss\n",
        "\n",
        "    def fit(self, X, y, loss=None):\n",
        "        \"\"\" Build decision tree \"\"\"\n",
        "        self.one_dim = len(np.shape(y)) == 1\n",
        "        self.root = self._build_tree(X, y)\n",
        "        self.loss=None\n",
        "\n",
        "    def _build_tree(self, X, y, current_depth=0):\n",
        "        \"\"\" Recursive method which builds out the decision tree and splits X and respective y\n",
        "        on the feature of X which (based on impurity) best separates the data\"\"\"\n",
        "\n",
        "        largest_impurity = 0\n",
        "        best_criteria = None    # Feature index and threshold\n",
        "        best_sets = None        # Subsets of the data\n",
        "\n",
        "        # Check if expansion of y is needed\n",
        "        if len(np.shape(y)) == 1:\n",
        "            y = np.expand_dims(y, axis=1)\n",
        "\n",
        "        # Add y as last column of X\n",
        "        Xy = np.concatenate((X, y), axis=1)\n",
        "\n",
        "        n_samples, n_features = np.shape(X)\n",
        "\n",
        "        if n_samples >= self.min_samples_split and current_depth <= self.max_depth:\n",
        "            # Calculate the impurity for each feature\n",
        "            for feature_i in range(n_features):\n",
        "                # All values of feature_i\n",
        "                feature_values = np.expand_dims(X[:, feature_i], axis=1)\n",
        "                unique_values = np.unique(feature_values)\n",
        "\n",
        "                # Iterate through all unique values of feature column i and\n",
        "                # calculate the impurity\n",
        "                for threshold in unique_values:\n",
        "                    # Divide X and y depending on if the feature value of X at index feature_i\n",
        "                    # meets the threshold\n",
        "                    Xy1, Xy2 = divide_on_feature(Xy, feature_i, threshold)\n",
        "\n",
        "                    if len(Xy1) > 0 and len(Xy2) > 0:\n",
        "                        # Select the y-values of the two sets\n",
        "                        y1 = Xy1[:, n_features:]\n",
        "                        y2 = Xy2[:, n_features:]\n",
        "\n",
        "                        # Calculate impurity\n",
        "                        impurity = self._impurity_calculation(y, y1, y2)\n",
        "\n",
        "                        # If this threshold resulted in a higher information gain than previously\n",
        "                        # recorded save the threshold value and the feature\n",
        "                        # index\n",
        "                        if impurity > largest_impurity:\n",
        "                            largest_impurity = impurity\n",
        "                            best_criteria = {\"feature_i\": feature_i, \"threshold\": threshold}\n",
        "                            best_sets = {\n",
        "                                \"leftX\": Xy1[:, :n_features],   # X of left subtree\n",
        "                                \"lefty\": Xy1[:, n_features:],   # y of left subtree\n",
        "                                \"rightX\": Xy2[:, :n_features],  # X of right subtree\n",
        "                                \"righty\": Xy2[:, n_features:]   # y of right subtree\n",
        "                                }\n",
        "\n",
        "        if largest_impurity > self.min_impurity:\n",
        "            # Build subtrees for the right and left branches\n",
        "            true_branch = self._build_tree(best_sets[\"leftX\"], best_sets[\"lefty\"], current_depth + 1)\n",
        "            false_branch = self._build_tree(best_sets[\"rightX\"], best_sets[\"righty\"], current_depth + 1)\n",
        "            return DecisionNode(feature_i=best_criteria[\"feature_i\"], threshold=best_criteria[\n",
        "                                \"threshold\"], true_branch=true_branch, false_branch=false_branch)\n",
        "\n",
        "        # We're at leaf => determine value\n",
        "        leaf_value = self._leaf_value_calculation(y)\n",
        "\n",
        "        return DecisionNode(value=leaf_value)\n",
        "\n",
        "\n",
        "    def predict_value(self, x, tree=None):\n",
        "        \"\"\" Do a recursive search down the tree and make a prediction of the data sample by the\n",
        "            value of the leaf that we end up at \"\"\"\n",
        "\n",
        "        if tree is None:\n",
        "            tree = self.root\n",
        "\n",
        "        # If we have a value (i.e we're at a leaf) => return value as the prediction\n",
        "        if tree.value is not None:\n",
        "            return tree.value\n",
        "\n",
        "        # Choose the feature that we will test\n",
        "        feature_value = x[tree.feature_i]\n",
        "\n",
        "        # Determine if we will follow left or right branch\n",
        "        branch = tree.false_branch\n",
        "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
        "            if feature_value >= tree.threshold:\n",
        "                branch = tree.true_branch\n",
        "        elif feature_value == tree.threshold:\n",
        "            branch = tree.true_branch\n",
        "\n",
        "        # Test subtree\n",
        "        return self.predict_value(x, branch)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\" Classify samples one by one and return the set of labels \"\"\"\n",
        "        y_pred = [self.predict_value(sample) for sample in X]\n",
        "        return y_pred\n",
        "\n",
        "    def print_tree(self, tree=None, indent=\" \"):\n",
        "        \"\"\" Recursively print the decision tree \"\"\"\n",
        "        if not tree:\n",
        "            tree = self.root\n",
        "\n",
        "        # If we're at leaf => print the label\n",
        "        if tree.value is not None:\n",
        "            print (tree.value)\n",
        "        # Go deeper down the tree\n",
        "        else:\n",
        "            # Print test\n",
        "            print (\"%s:%s? \" % (tree.feature_i, tree.threshold))\n",
        "            # Print the true scenario\n",
        "            print (\"%sT->\" % (indent), end=\"\")\n",
        "            self.print_tree(tree.true_branch, indent + indent)\n",
        "            # Print the false scenario\n",
        "            print (\"%sF->\" % (indent), end=\"\")\n",
        "            self.print_tree(tree.false_branch, indent + indent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBt9hG7BJIyL"
      },
      "source": [
        "## Regression Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohmyRj_GGilC"
      },
      "outputs": [],
      "source": [
        "class RegressionTree(DecisionTree):\n",
        "\n",
        "    def _calculate_variance_reduction(self, y, y1, y2):\n",
        "        var_tot = calculate_variance(y)\n",
        "        var_1 = calculate_variance(y1)\n",
        "        var_2 = calculate_variance(y2)\n",
        "        frac_1 = len(y1) / len(y)\n",
        "        frac_2 = len(y2) / len(y)\n",
        "\n",
        "        # Calculate the variance reduction\n",
        "        variance_reduction = var_tot - (frac_1 * var_1 + frac_2 * var_2)\n",
        "\n",
        "        return sum(variance_reduction)\n",
        "\n",
        "    def _mean_of_y(self, y):\n",
        "        value = np.mean(y, axis=0)\n",
        "        return value if len(value) > 1 else value[0]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self._impurity_calculation = self._calculate_variance_reduction\n",
        "        self._leaf_value_calculation = self._mean_of_y\n",
        "        super(RegressionTree, self).fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOVcANGaJMII"
      },
      "source": [
        "## Gradient Boosting model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBCZp_sKD8TJ"
      },
      "outputs": [],
      "source": [
        "class GradientBoosting(object):\n",
        "    \"\"\"Super class of GradientBoostingClassifier and GradientBoostinRegressor. \n",
        "    Uses a collection of regression trees that trains on predicting the gradient\n",
        "    of the loss function. \n",
        "    Parameters:\n",
        "    -----------\n",
        "    n_estimators: int\n",
        "        The number of classification trees that are used.\n",
        "    learning_rate: float\n",
        "        The step length that will be taken when following the negative gradient during\n",
        "        training.\n",
        "    min_samples_split: int\n",
        "        The minimum number of samples needed to make a split when building a tree.\n",
        "    min_impurity: float\n",
        "        The minimum impurity required to split the tree further. \n",
        "    max_depth: int\n",
        "        The maximum depth of a tree.\n",
        "    regression: boolean\n",
        "        True or false depending on if we're doing regression or classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, min_samples_split=2,\n",
        "                 min_impurity=1e-7, max_depth=3, regression=False):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_impurity = min_impurity\n",
        "        self.max_depth = max_depth\n",
        "        self.regression = regression\n",
        "        #self.bar = progressbar.ProgressBar(widgets=bar_widgets)\n",
        "        \n",
        "        # Square loss for regression\n",
        "        # Log loss for classification\n",
        "        self.loss = SquareLoss()\n",
        "        if not self.regression:\n",
        "            self.loss = CrossEntropy()\n",
        "\n",
        "        # Initialize regression trees\n",
        "        self.trees = []\n",
        "        for i in range(n_estimators):\n",
        "            tree = RegressionTree(\n",
        "                    min_samples_split=self.min_samples_split,\n",
        "                    min_impurity=self.min_impurity,\n",
        "                    max_depth=self.max_depth)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
        "        y_pred = np.full(np.shape(y), np.mean(y, axis=0))\n",
        "        #for i in self.bar(range(self.n_estimators)):\n",
        "        for i in range(self.n_estimators):\n",
        "            gradient = self.loss.gradient(y, y_pred)\n",
        "            self.trees[i].fit(X, gradient)\n",
        "            update = self.trees[i].predict(X)\n",
        "            # Update y prediction\n",
        "            y_pred -= np.multiply(self.learning_rate, update)\n",
        "        \n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.array([])\n",
        "        # Make predictions\n",
        "        for tree in self.trees:\n",
        "            update = tree.predict(X)\n",
        "            update = np.multiply(self.learning_rate, update)\n",
        "            y_pred = -update if not y_pred.any() else y_pred - update\n",
        "\n",
        "        if not self.regression:\n",
        "            '''\n",
        "            # Turn into probability distribution\n",
        "            print(np.exp(y_pred))\n",
        "            print(np.sum(np.exp(y_pred)))\n",
        "            print(np.expand_dims(np.sum(np.exp(y_pred)), axis=1))\n",
        "            y_pred = np.exp(y_pred) / np.expand_dims(np.sum(np.exp(y_pred)), axis=1)\n",
        "            \n",
        "\n",
        "            \n",
        "            # Set label to the value that maximizes probability\n",
        "            #y_pred = np.argmax(y_pred, axis=1)\n",
        "            #print(y_pred)\n",
        "            \n",
        "            '''\n",
        "            y_pred_list = y_pred.tolist()\n",
        "            temp = []\n",
        "            for i in y_pred_list:\n",
        "                if i>1:\n",
        "                    temp.append(1)\n",
        "                else:   temp.append(0)\n",
        "            y_pred = np.array(temp)\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AxqaDfcvdgg"
      },
      "source": [
        "## Run Gradient Boosting with Our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp12O0L_G4Of",
        "outputId": "c675817a-ecdb-4532-f410-32c2a01b872f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient Boosting Accuarcy score:  0.8\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "clf_gbc = GradientBoosting(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "clf_gbc.fit(train_df_data, train_df_target)\n",
        "test_preds = clf_gbc.predict(test_df_data)\n",
        "\n",
        "print('Gradient Boosting Accuarcy score: ', accuracy_score(test_df_target, test_preds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXVvunrwvn-B"
      },
      "source": [
        "## Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNQusBq59OWj",
        "outputId": "b54e556b-48d3-4010-c3c6-1ab5cba2f029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best Gradient Boosting accuracy:  0.8\n",
            "best n estimators:  5\n",
            "best max depth:  1\n",
            "best learning rate 0.01\n"
          ]
        }
      ],
      "source": [
        "parameters = {\n",
        "    \"n_estimators\":[5,10,15,20,25,30,45,50,100,200],\n",
        "    \"max_depth\":[1,2,3,4,5],\n",
        "    \"learning_rate\":[0.01,0.05,0.1,0.25,0.5,1]\n",
        "}\n",
        "max_acc_score = 0\n",
        "best_n_estimators = 5\n",
        "for n_estimators in parameters[\"n_estimators\"]:\n",
        "    clf_gbc = GradientBoosting(n_estimators=n_estimators)\n",
        "    clf_gbc.fit(train_df_data, train_df_target)\n",
        "    test_preds = clf_gbc.predict(test_df_data)\n",
        "    acc = accuracy_score(test_df_target, test_preds)\n",
        "    if acc > max_acc_score:\n",
        "          max_acc_score = acc\n",
        "          best_n_estimators = n_estimators\n",
        "best_max_depth = 1\n",
        "for max_depth in parameters[\"max_depth\"]:\n",
        "    clf_gbc = GradientBoosting(n_estimators=best_n_estimators, max_depth=max_depth)\n",
        "    clf_gbc.fit(train_df_data, train_df_target)\n",
        "    test_preds = clf_gbc.predict(test_df_data)\n",
        "    acc = accuracy_score(test_df_target, test_preds)\n",
        "    if acc > max_acc_score:\n",
        "          max_acc_score = acc\n",
        "          best_max_depth = max_depth\n",
        "\n",
        "best_learning_rate = 0.01\n",
        "for learning_rate in parameters[\"learning_rate\"]:\n",
        "    clf_gbc = GradientBoosting(n_estimators=best_n_estimators, learning_rate=learning_rate, max_depth=best_max_depth)\n",
        "    clf_gbc.fit(train_df_data, train_df_target)\n",
        "    test_preds = clf_gbc.predict(test_df_data)\n",
        "    acc = accuracy_score(test_df_target, test_preds)\n",
        "    if acc > max_acc_score:\n",
        "          max_acc_score = acc\n",
        "          best_learning_rate = learning_rate\n",
        "print('best Gradient Boosting accuracy: ',max_acc_score)\n",
        "print('best n estimators: ',best_n_estimators)\n",
        "print('best max depth: ',best_max_depth)\n",
        "print('best learning rate',best_learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfYdlo7Av70L"
      },
      "source": [
        "## Compare our model with sklearn.ensemble GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pcz9gWdBUlb3",
        "outputId": "0d8e634d-f027-43bf-bc24-848c1488376f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sklearn Gradient Boosting accuarcy score:  0.79\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "clf_gbc_sklearn = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
        "     max_depth=1, random_state=0).fit(train_df_data, train_df_target)\n",
        "test_preds = clf_gbc_sklearn.predict(test_df_data)\n",
        "print('sklearn Gradient Boosting accuarcy score: ', accuracy_score(test_df_target, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gkxgopmHSMd",
        "outputId": "37b879a0-d9d4-4564-9c38-44b20f5a06b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best sklearn Gradient Boosting accuracy:  0.83\n",
            "best n estimators:  25\n",
            "best max depth:  1\n",
            "best learning rate 0.01\n"
          ]
        }
      ],
      "source": [
        "parameters = {\n",
        "    \"n_estimators\":[5,10,15,20,25,30,45,50],\n",
        "    \"max_depth\":[1,2,3,4,5],\n",
        "    \"learning_rate\":[0.01,0.05,0.1,0.25,0.5,1]\n",
        "}\n",
        "max_acc_score = 0\n",
        "best_n_estimators = 5\n",
        "for n_estimators in parameters[\"n_estimators\"]:\n",
        "    clf_gbc_sklearn = GradientBoostingClassifier(n_estimators=n_estimators)\n",
        "    clf_gbc_sklearn.fit(train_df_data, train_df_target)\n",
        "    test_preds = clf_gbc_sklearn.predict(test_df_data)\n",
        "    acc = accuracy_score(test_df_target, test_preds)\n",
        "    if acc > max_acc_score:\n",
        "          max_acc_score = acc\n",
        "          best_n_estimators = n_estimators\n",
        "best_max_depth = 1\n",
        "for max_depth in parameters[\"max_depth\"]:\n",
        "    clf_gbc_sklearn = GradientBoostingClassifier(n_estimators=best_n_estimators, max_depth=max_depth)\n",
        "    clf_gbc_sklearn.fit(train_df_data, train_df_target)\n",
        "    test_preds = clf_gbc_sklearn.predict(test_df_data)\n",
        "    acc = accuracy_score(test_df_target, test_preds)\n",
        "    if acc > max_acc_score:\n",
        "          max_acc_score = acc\n",
        "          best_max_depth = max_depth\n",
        "\n",
        "best_learning_rate = 0.01\n",
        "for learning_rate in parameters[\"learning_rate\"]:\n",
        "    clf_gbc_sklearn = GradientBoostingClassifier(n_estimators=best_n_estimators, learning_rate=learning_rate, max_depth=best_max_depth)\n",
        "    clf_gbc_sklearn.fit(train_df_data, train_df_target)\n",
        "    test_preds = clf_gbc_sklearn.predict(test_df_data)\n",
        "    acc = accuracy_score(test_df_target, test_preds)\n",
        "    if acc > max_acc_score:\n",
        "          max_acc_score = acc\n",
        "          best_learning_rate = learning_rate\n",
        "print('best sklearn Gradient Boosting accuracy: ',max_acc_score)\n",
        "print('best n estimators: ',best_n_estimators)\n",
        "print('best max depth: ',best_max_depth)\n",
        "print('best learning rate',best_learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqbp2bMaf-Qo"
      },
      "source": [
        "## Supplementary\n",
        "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kiTzGvmf-Qo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "CDRCD35qf-QZ",
        "WvFqH59Zf-Qd",
        "Mkcuyi1Jf-Qh",
        "UsR44nLef-Qj",
        "i8_TjqUff-Qm",
        "f6mdWSFzf-Qm"
      ],
      "name": "HW3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
